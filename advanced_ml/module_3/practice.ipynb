{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Данные\n",
    "x1 = np.array([1, 2, 3, 4, 5])\n",
    "x2 = np.array([6, 7, 8, 9, 10])\n",
    "y = np.array([11, 12, 13, 14, 15])\n",
    "\n",
    "# Объединение признаков\n",
    "X = np.column_stack((x1, x2))\n",
    "\n",
    "# Обучение модели линейной регрессии\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Вывод вектора весов модели\n",
    "weights = reg.coef_\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "x1 = np.array([1, 2, 3, 4, 5])\n",
    "x2 = np.array([6, 7, 8, 9, 10])\n",
    "y = np.array([11, 12, 13, 14, 15])\n",
    "reg = LinearRegression()\n",
    "\n",
    "X = np.column_stack((x1, x2))\n",
    "reg.fit(X, y)\n",
    "y_pred = reg.predict(np.array([[5, 6]]))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "f1 = round(f1_score(y_test, y_pred),2)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=41) \n",
    "model = DecisionTreeClassifier(max_depth=3, min_samples_split=5)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy случайного леса: 1.0\n",
      "Accuracy градиентного бустинга: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred_forest = forest.predict(X_test)\n",
    "\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "grad_boost.fit(X_train, y_train)\n",
    "y_pred_grad_boost = grad_boost.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, y_pred_forest)\n",
    "gb_acc = accuracy_score(y_test, y_pred_grad_boost)\n",
    "\n",
    "print(\"Accuracy случайного леса:\", round(rf_acc, 2))\n",
    "print(\"Accuracy градиентного бустинга:\", round(gb_acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8039633  0.57026999]\n",
      " [0.18520943 0.72228065]\n",
      " [0.36376248 0.20008043]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadezdaoskina/.pyenv/versions/3.10.9/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit_predict(X)\n",
    "center_coords = model.cluster_centers_\n",
    "print(center_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Центроиды кластеров:\n",
      "[[0.8039633  0.57026999]\n",
      " [0.36376248 0.20008043]\n",
      " [0.18520943 0.72228065]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadezdaoskina/.pyenv/versions/3.10.9/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Генерация случайных данных\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)  # 100 точек в 2-мерном пространстве\n",
    "\n",
    "# Инициализация и обучение модели k-means\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "model.fit(X)\n",
    "\n",
    "# Получение центроидов кластеров\n",
    "center_coords = model.cluster_centers_\n",
    "print(\"Центроиды кластеров:\")\n",
    "print(center_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7342129315790209\n",
      "0.7588498590551315\n",
      "0.5981066468725696\n",
      "0.7529063638339899\n",
      "0.8118811138698677\n",
      "0.7908766199383688\n",
      "0.7186103227487628\n",
      "0.7561768145562359\n",
      "0.6812742746955424\n",
      "0.718017227342148\n",
      "0.7320912174491638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_blobs(n_samples=100, n_features=4, centers=4, random_state=42)\n",
    "silhouette_avg = []\n",
    "for i in range(10):\n",
    "    X, y = make_blobs(n_samples=100, n_features=4, centers=4, random_state=i)\n",
    "    linkage_matrix = linkage(X, method='ward')\n",
    "    labels = fcluster(linkage_matrix, t=4, criterion='maxclust')\n",
    "    current_silhouette = silhouette_score(X, labels)\n",
    "    silhouette_avg.append(current_silhouette)\n",
    "    print(current_silhouette)\n",
    "\n",
    "silhouette_avg = np.mean(silhouette_avg)\n",
    "print(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10, 0.8229345005215012]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=3, random_state=42)\n",
    "\n",
    "def dbscan_silhouette(eps, min_samples):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Проверка на наличие менее чем 2 кластеров\n",
    "    if len(set(labels)) <= 1:\n",
    "        return -1\n",
    "\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "best_eps, best_min_samples, best_silhouette = None, None, -1\n",
    "for eps in [0.1, 0.5, 1]:\n",
    "    for min_samples in [5, 10, 20]:\n",
    "        current_silhouette = dbscan_silhouette(eps, min_samples)  # Используем уникальное имя\n",
    "        if current_silhouette > best_silhouette:\n",
    "            best_silhouette = current_silhouette\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples\n",
    "        \n",
    "print([best_eps, best_min_samples, best_silhouette])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "np.random.seed(42)\n",
    "# Создание искусственных данных\n",
    "data = np.random.rand(1000, 10)\n",
    "# Выполнение алгоритма PCA для сокращения размерности до 3-х компонент\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(data)\n",
    "transformed_data = pca.transform(data)\n",
    "# Анализ факторной нагрузки\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "# Выбор наиболее важных факторов\n",
    "most_important_factors = np.argsort(np.abs(loadings[:,\n",
    "0]))[::-1][:3]\n",
    "# Вычисление суммы весов наиболее важных факторов\n",
    "sum_weights = np.sum(np.abs(loadings[most_important_factors, 0]))\n",
    "# Округление ответа до двух знаков после запятой\n",
    "sum_weights_rounded = round(sum_weights, 2)\n",
    "# Вывод результата\n",
    "print(sum_weights_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(42)\n",
    "\n",
    "# Создание матрицы рейтингов\n",
    "ratings = np.random.randint(1, 6, size=(5, 5))\n",
    "\n",
    "# Разложение матрицы с помощью SVD\n",
    "U, S, Vt = svd(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.51353895,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  4.16875468,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  2.77482398,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.78099678,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.16074933]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный рейтинг для пользователя 2 и товара 4: 2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Создаем матрицу рейтингов пользователей и товаров\n",
    "R = np.array([\n",
    "    [3, 1, 2, 3],\n",
    "    [4, 3, 4, 3],\n",
    "    [2, 2, 1, 5],\n",
    "    [1, 5, 5, 2]\n",
    "])\n",
    "\n",
    "# Для примера будем считать, что нам нужно предсказать рейтинг для пользователя 2 и товара 4\n",
    "user_id = 2\n",
    "item_id = 4\n",
    "\n",
    "# Ищем среднее значение рейтингов для каждого товара и вычитаем его из матрицы рейтингов\n",
    "item_means = np.mean(R, axis=0)\n",
    "R_norm = R - item_means\n",
    "\n",
    "# Вычисляем сингулярное разложение матрицы рейтингов\n",
    "U, s, Vt = np.linalg.svd(R_norm)\n",
    "\n",
    "# Определяем размерность матрицы рейтингов и уменьшаем размерность сингулярным разложением\n",
    "n_users, n_items = R_norm.shape\n",
    "n_components = 2  # Количество главных компонент, которые мы оставляем\n",
    "\n",
    "# Сужение размерности\n",
    "U_red = U[:, :n_components]  # Берем первые n_components столбцов матрицы U\n",
    "S_red = np.diag(s[:n_components])  # Создаем диагональную матрицу с первыми n_components сингулярными значениями\n",
    "Vt_red = Vt[:n_components, :]  # Берем первые n_components строк из матрицы Vt\n",
    "\n",
    "# Восстанавливаем приближенную матрицу рейтингов\n",
    "R_pred = np.dot(np.dot(U_red, S_red), Vt_red) + item_means\n",
    "\n",
    "# Округляем предсказание до одной десятой\n",
    "rating_pred = round(R_pred[user_id-1, item_id-1], 1)\n",
    "print(f\"Предсказанный рейтинг для пользователя {user_id} и товара {item_id}: {rating_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный рейтинг для пользователя 2 и товара 4: 3.3\n"
     ]
    }
   ],
   "source": [
    "# Создаем матрицу рейтингов пользователей и товаров\n",
    "R = np.array([[3, 1, 2, 3],\n",
    "[4, 3, 4, 3],\n",
    "[2, 2, 1, 5],\n",
    "[1, 5, 5, 2]])\n",
    "# Для примера будем считать, что нам нужно предсказать рейтинг для пользователя 2 и товара 4\n",
    "user_id = 2\n",
    "item_id = 4\n",
    "# Ищем среднее значение рейтингов для каждого товара и вычитаем его из матрицы рейтингов\n",
    "item_means = np.mean(R, axis=0)\n",
    "R_norm = R - item_means\n",
    "# Вычисляем сингулярное разложение матрицы рейтингов\n",
    "U, s, Vt = np.linalg.svd(R_norm)\n",
    "# Определяем размерность матрицы рейтингов и уменьшаем размерность сингулярным разложением\n",
    "n_users, n_items = R_norm.shape\n",
    "n_components = 2 # Количество главных компонент, которые мы оставляем\n",
    "S = np.diag(s)[:, :n_components]\n",
    "U_red = U[:, :n_components]\n",
    "Vt_red = Vt[:n_components, :]\n",
    "R_pred = np.dot(np.dot(U_red.T, S), Vt_red) + item_means\n",
    "# Округляем предсказание до одной десятой\n",
    "rating_pred = round(R_pred[user_id-1, item_id-1], 1)\n",
    "print(f\"Предсказанный рейтинг для пользователя {user_id} и товара {item_id}: {rating_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный рейтинг для пользователя 2 и товара 4: 3.3\n"
     ]
    }
   ],
   "source": [
    "# Создаем матрицу рейтингов пользователей и товаров\n",
    "R = np.array([[3, 1, 2, 3],\n",
    "[4, 3, 4, 3],\n",
    "[2, 2, 1, 5],\n",
    "[1, 5, 5, 2]])\n",
    "# Для примера будем считать, что нам нужно предсказать рейтинг для пользователя 2 и товара 4\n",
    "user_id = 2\n",
    "item_id = 4\n",
    "# Ищем среднее значение рейтингов для каждого товара и вычитаем его из матрицы рейтингов\n",
    "item_means = np.mean(R, axis=0)\n",
    "R_norm = R - item_means\n",
    "# Вычисляем сингулярное разложение матрицы рейтингов\n",
    "U, s, Vt = np.linalg.svd(R_norm)\n",
    "# Определяем размерность матрицы рейтингов и уменьшаем размерность сингулярным разложением\n",
    "n_users, n_items = R_norm.shape\n",
    "n_components = 2 # Количество главных компонент, которые мы оставляем\n",
    "S = np.diag(s)[:, :n_components]\n",
    "U_red = U[:, :n_components]\n",
    "Vt_red = Vt[:n_components, :]\n",
    "R_pred = np.dot(np.dot(U_red.T, S), Vt_red) + item_means\n",
    "# Округляем предсказание до одной десятой\n",
    "rating_pred = round(R_pred[user_id-1, item_id-1], 1)\n",
    "print(f\"Предсказанный рейтинг для пользователя {user_id} и товара {item_id}: {rating_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.  0.  2.]\n",
      " [ 8. 10. 12.]\n",
      " [18. 20. 22.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# создаем исходную матрицу A размером 5x5\n",
    "A = np.array([[1, 2, 3, 4, 5], \n",
    "              [6, 7, 8, 9, 10], \n",
    "              [11, 12, 13, 14, 15], \n",
    "              [16, 17, 18, 19, 20], \n",
    "              [21, 22, 23, 24, 25]])\n",
    "\n",
    "# создаем матрицу фильтра F размером 3x3\n",
    "F = np.array([[1, 2, -1], \n",
    "              [1, 0, -1], \n",
    "              [1, 0, -1]])\n",
    "\n",
    "# определяем размеры матриц A и F\n",
    "height_a, width_a = A.shape\n",
    "height_f, width_f = F.shape\n",
    "\n",
    "# инициализируем выходную матрицу C\n",
    "C = np.zeros((3,3))\n",
    "\n",
    "# проходим по каждой строке матрицы A\n",
    "for i in range(3):\n",
    "    # проходим по каждому столбцу матрицы A\n",
    "    for j in range(3):\n",
    "        # получаем окно размером 3x3, начиная с текущей позиции\n",
    "        window = A[i: i+3, j: j+3]\n",
    "        # перемножаем окно с матрицей фильтра\n",
    "        result = window * F\n",
    "        # суммируем результаты умножения, чтобы получить новое значение в выходной матрице C\n",
    "        C[i, j] = np.sum(result)\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднеквадратичная ошибка: 1.2729\n"
     ]
    }
   ],
   "source": [
    "# Создаем искусственные данные\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Входные данные (матрица 100x5)\n",
    "X = np.random.randn(100, 5)\n",
    "\n",
    "# Определяем архитектуру нейронной сети\n",
    "input_size = 5  # Входной размер\n",
    "hidden_size = 3  # Количество нейронов в скрытом слое\n",
    "output_size = 1  # Выходной размер\n",
    "\n",
    "# Инициализируем веса случайным образом\n",
    "W1 = np.random.randn(input_size, hidden_size)  # матрица весов для входного слоя\n",
    "W2 = np.random.randn(hidden_size, output_size)  # матрица весов для скрытого слоя\n",
    "\n",
    "# Определяем функцию активации (сигмоид)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Прямой проход по нейронной сети\n",
    "hidden_layer = sigmoid(np.dot(X, W1))  # Расчет активаций скрытого слоя\n",
    "output_layer = sigmoid(np.dot(hidden_layer, W2))  # Расчет выходного слоя\n",
    "\n",
    "# Реальные значения (истинные метки)\n",
    "y_true = np.random.randn(100, 1)\n",
    "\n",
    "# Вычисляем среднеквадратичную ошибку\n",
    "mse = np.mean((y_true - output_layer) ** 2)\n",
    "\n",
    "# Выводим ошибку\n",
    "print(f\"Среднеквадратичная ошибка: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n"
     ]
    }
   ],
   "source": [
    "# Создаем искусственные данные\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.randn(100, 5)  # матрица 100x5\n",
    "\n",
    "# Определяем архитектуру нейронной сети\n",
    "input_size = 5\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Инициализируем веса случайным образом\n",
    "W1 = np.random.randn(5, 3)  # матрица весов 5x3\n",
    "W2 = np.random.randn(3, 1)  # матрица весов 3x1\n",
    "\n",
    "# Определяем функцию активации\n",
    "def sigmoid(x):\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Прямой проход по нейронной сети\n",
    "hidden_layer = sigmoid(np.dot(X, W1))  # скрытый слой\n",
    "output_layer = np.dot(hidden_layer, W2)  # выходной слой\n",
    "\n",
    "# Вычисляем среднеквадратичную ошибку\n",
    "y_true = np.random.randn(100, 1)  # реальные значения\n",
    "mse = np.round(np.mean((y_true - output_layer) ** 2),1)\n",
    "\n",
    "print(mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
